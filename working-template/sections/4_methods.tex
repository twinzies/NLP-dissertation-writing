% Methods
% Methods: The experimental approach, including descriptions of metrics, baseline models, etc. Details about hyperparameters, optimization choices, etc., are probably best given in appendices, unless they are central to the arguments.
% Mention The epistemic limits of interpreting LLM (or human) behavior through psychosocial theories here or in the discussion section.
\section{Methods} \label{sec:Methods}
% ~1200 words ≈ ~9–10 paragraphs

\subsection{Data collection and preprocessing}
% This section can describe text processing steps - how the human responses were selected and which posts were selected from the RedArcs dataset etc.
A dataset was built from the RedditArchives for two public subreddits—AITAH, and Anxiety. For each subreddit, the top 1,000 most upvoted posts were selected, excluding weekly megathreads, deleted/removed items, and AutoModerator entries. For every retained post we extracted (i) the most upvoted comment and (ii) the comment that the OP engaged with most; all artifacts were saved to standardized CSVs for downstream analysis.

Text was cleaned with minimal, semantics-preserving preprocessing: we removed non-English items, de-identified obvious personal identifiers (usernames, emails, links to personal sites), standardized whitespace and Unicode characters, and lightly constrained length (posts ~50–500 words; comments ~5–300 words) for comparability. 

We treat each Reddit thread (the post and its comments) as a single analytic unit during sampling, manual checks, and statistic aggregation. This preserves thread integrity and prevents dependence-induced bias when comparing human and LLM responses drawn from the same conversation. We also removed exact and near-duplicate texts (specifically, crossposts, copypastes and bot repeats) to prevent inflated counts and biased comparisons.

\smallskip Prompts for each step in the evaluation and generation subsystems are provided in the appendices \ref{sec:app_prompts}.

\subsection{Procedures}
% This section should discuss the creation of proxies for empathy and sycophancy.
For each selected post, we prompt the target language model firstly, with the base prompt \ref{sec:app_prompts} to establish a \textbf{baseline open-ended response} to the body of the post. This response is appended to a table containing: (i) the model-generated response, (ii) the top upvoted human comment, and (iii) the most engaged human comment (available for approximately half of the posts). The resulting dataframe consists of the original post body, paired with two types of responses to personal queries - human and AI responses.

\medskip 
\textbf{Feature Extraction}

\smallskip 
\begin{itemize}
    \item Features are extracted at the sentence level, consisting of sentences from both the responses and post bodies that are annotated in accordance with steps 3 and 4 of the Evaluation Framework \ref{pipeline_steps}.
    \item Note that each feature is annotated with:
    \begin{itemize}
        \item \textbf{a.} \textbf{Values exhibited} by the author.
        \item \textbf{b.} \textbf{Values incentivized} by the author of the response.
    \end{itemize}
\end{itemize}

While RQ2 concerns drawing inter- and intra-paradigm comparative insights across the four psychosocial frameworks, sub-research question RQ2a addresses the epistemic limits of interpreting LLM behavior through psychosocial theories. Specifically, the same feature may be perceived as 'sycophantic' under Goffman’s ToF, 'empathic' under Rogerian PCT.

To support these inquiries, the file saved by the evaluation pipeline in step 5 consists of a dataframe that records: the original post, the set of extracted features for each of the 2 different types of responses (human response, language models) and the values either exhibited or incentivized by each feature within any of the four applicable psychosocial paradigm(s).

This analytical dataset forms the basis for the subsequent analyses necessary to address RQ3, where we analyze the differences in distributions of values in the responses obtained from reddit authored by humans compared to the language model produced responses to personal queries.

\subsection{Experiments}
The experimental design spans two major dimensions: (i) response type (two forms of human responses and three language model responses) and (ii) domains (two distinct subreddits of interest in personal queries - social with r/aitah and psychosocial r/anxiety).

\textbf{Experiment 1} is designed to compare the distributions of values across two different response categories i. human authored, and ii. LLM authored responses. 

An analysis of the comparison is done based on both the explicit values expressed by the respondent and the implicit values incentivized by the language of the response under each of the four psychosocial paradigms.

Statistical Testing: The annotated dataset is then used to construct contingency tables and perform chi-square tests to assess independence between intra- and inter-paradigm values.

The metrics thus obtained are used to inform the analysis on how the relationships between inter-paradigm values differ between human- and LLM-authored responses.

In \textbf{Experiment 2}, the focus of the evaluation is to understand how variations in prompt design influence the breadth of values expressed by the LLM. Specifically,
how the same statement is annotated differently under the selected suite of psychosocial paradigms.

we incorporate prompts that explicitly instruct the model to (i) generate a response most likely to be upvoted, and (ii) generate a response most likely to engage the author.


\subsubsection{Generations}
A set of targeted experiments are run with DeepReflect’s analyses to investigate the efficacy of control mechanisms to align the values in language model outputs more closely with those observed in human responses. The generation experiments are implemented using the following methods:

\begin{enumerate}
%     \item \textbf{With supervised fine-tuning (SFT)} [model: GPT-x, Paradigms: Empathy, Sycophancy]
% Experiments with Fine-tuning the language model on two synthetic datasets, generated to reflect (i) sycophantic and (ii) empathic behaviors.
% Additional experiments with temperature scaling for the Roger's PCT paradigm.

    \item \textbf{Chain-of-thought reasoning} [models: Claude; one of Qwen-3 or LLaMA-3.1; paradigms: Rogers PCT and RVS]
Prompt augmentation experiments, where values with low frequency in LLM responses are explicitly introduced and emphasized (e.g., Rogers PCT: Unconditional positive regard, Psychological freedom; RVS: A comfortable life).
\end{enumerate}

% \begin{enumerate}
%     \item \textbf{With supervised fine-tuning (SFT)}: We use the extracted distributions to generate synthetic responses that mirror human communication patterns, creating training data for supervised fine-tuning of language models on supportive communication.
    
%     \item \textbf{Chain-of-thought prompting}: The identified patterns inform chain-of-thought prompting techniques for GPT-4 and Claude, guiding these models to produce responses with appropriate levels of empathy, advice-giving, and self-disclosure for each community context.
% \end{enumerate}

\subsection{Construct Validity and Evaluation Metrics}
To assess construct validity, one human annotator labeled 100 randomly sampled post–response pairs across all four paradigms for each response type. The PCT framework encompasses 15 behaviors, Goffman’s ToF 5, the RVS 36, and Anthropic’s Value Tree 18.

Inter-rater reliability reached Cohen’s $\kappa$ above xx for all metrics, with an overall classification accuracy of yy. For the AITAH dataset, verdicts and accompanying statements in responses were used as proxies for Empathy and Sycophancy, each mapped onto five behaviors as defined by their respective theoretical traditions\footnote{This strategy is conceptually aligned with prior work on social sycophancy \cite{cheng-etal-sycophancy}}.


For the RVS and Anthropic Value Tree frameworks, which yield categorical distributions rather than binary judgments, pairwise error rates such as False Negative Rate (FNR) and False Positive Rate (FPR) are not directly applicable.
To identify significant associations between features annotated under more than one distinct paradigm we construct contingency tables and use chi-square analysis with further details provided in section~\ref{sec:Analysis}. 
% TODO: Add some math here for the chi-square analysis.


% Users of older versions of \LaTeX{} may encounter the following error during compilation: 
% \begin{quote}
% \tt\verb|\pdfendlink| ended up in different nesting level than \verb|\pdfstartlink|.
% \end{quote}
% This happens when pdf\LaTeX{} is used and a citation splits across a page boundary. The best way to fix this is to upgrade \LaTeX{} to 2018-12-01 or later.