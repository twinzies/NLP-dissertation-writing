% Methods
% Methods: The experimental approach, including descriptions of metrics, baseline models, etc. Details about hyperparameters, optimization choices, etc., are probably best given in appendices, unless they are central to the arguments.
% Mention The epistemic limits of interpreting LLM (or human) behavior through psychosocial theories here or in the discussion section.
\section{Methods} \label{sec:Methods}
% ~1200 words ≈ ~9–10 paragraphs

\subsection{Data preprocessing}
% This section can describe text processing steps - how the human responses were selected and which posts were selected from the RedArcs dataset etc.
A dataset was built from the RedditArchives for two public subreddits—AITAH, and Anxiety. For each subreddit, the top 1,000 most upvoted posts were selected, excluding weekly megathreads, deleted/removed items, and AutoModerator entries. We also removed exact and near-duplicate texts (specifically, crossposts, copypastes and bot repeats) to prevent inflated counts and biased comparisons.

For every retained post we extracted (i) the most upvoted comment and (ii) the comment that the OP engaged with most; all artifacts were saved to standardized CSVs for downstream analysis.

Text was cleaned with minimal, semantics-preserving preprocessing: we removed non-English items, de-identified obvious personal identifiers (usernames, emails, links to personal sites), standardized whitespace and Unicode characters, and lightly constrained length (posts ~50–500 words; comments ~5–300 words) for comparability.

We treat each set of post and human-authored responses in a Reddit thread as a single analytical unit during stratified sampling.
and each feature within the set (the post body and its responses) as a single analytical unit during manual checks, and statistical aggregation.
% This preserves thread integrity and prevents dependence-induced bias when comparing human and LLM responses drawn from the same conversation.

\subsection{Procedures}
% This section should discuss the creation of proxies for empathy and sycophancy.
For each selected post, we prompt the target language model firstly, with the base prompt\footnote{\smallskip Prompts for this step are provided in the appendices \ref{sec:app_prompts}.} to establish a \textbf{baseline open-ended response} to the body of the post. This response is appended to a table containing: (i) the model-generated response, (ii) the top upvoted human comment, and (iii) the most engaged human comment (available for approximately half of the posts). The resulting dataframe consists of the original post body, paired with two types of responses to personal queries - human and AI responses.

\medskip 
\textbf{Feature Extraction}

\smallskip 
\begin{itemize}
    \item In steps 3 and 4 of the Evaluation Framework (Figure~\ref{pipeline_steps}), features\footnote{Note that in this context, 'feature' refers to the part of the text used for annotation from the responses and the body of the post.} are annotated at the sentence level within each body–response pair. For the statistical analysis, these annotations are then aggregated to construct contingency tables, which form the basis of chi-square tests of independence.
    \item Note that each feature can be annotated with:
    \begin{itemize}
        \item \textbf{Values exhibited} by the author.
        \item \textbf{Values incentivized} by the author of the response. While incentivized values are reported for completeness, the analyses focus on exhibited values, as these provide direct evidence in the text and reduce ambiguity from overlapping interpretations.
    \end{itemize}
\end{itemize}

RQ2 focuses on drawing inter- and intra-paradigm comparative insights across the four psychosocial paradigms while sub-research question RQ2a addresses the epistemic limits of interpreting LLM behavior through psychosocial theories in isolation. Specifically, the same feature may be perceived as 'sycophantic' under Goffman’s ToF, 'empathic' under Rogerian PCT.

To support these inquiries, the file saved by the evaluation pipeline in step 5 consists of: the annotated features of the original post, annotated features within the set of the 2 different types of responses (human response, language models) for values exhibited or incentivized under the relevant four psychosocial paradigm(s).

This annotated dataset forms the basis for the subsequent analyses necessary to also address RQ3, which studies the differences in the distributions of values between human-authored and language model–generated responses to personal queries.

\subsection{Experiments}
% TODO: review wording below for clarity and grammar.
The experimental design spans two major dimensions: (i) qualitative analysis of the sentence-level features (ii) quantification of the verdicts in the features by source type (two forms of human responses and three language model responses). While i. is conducted for each of the two datasets (r/aita and r/anxiety), under the four psychosocial paradigms, ii. is valid only for the r/aita dataset, where the responses may contain explicit judgments or not - forming 3 distinct classes (NTA, YTA, No judgment).

\subsubsection{Experiment 1}
In \textbf{Experiment 1}, the primary objective is to compare the selected paradigms and analyze the distributions of values across them, with the aim of ultimately determining how paradigm choice can lead to divergent interpretations of the same LLM response.

While values incentivized are also provided in the results, the analyses are focused on \textbf{values exhibited} under each paradigm by the two sources of response.

\smallskip \textbf{Statistical Methods}

The annotated dataset is used to construct contingency tables that shows how two categorical variables co-occur (with the values of a selected paradigm 1 represented across the columns and the values of the second paradigm represented across the rows). Chi-square tests are performed to assess independence between intra- and inter-paradigm values. The Benferroni correction is applied to control the family-wise error rate.

% Additional experiments: We incorporate prompts that explicitly instruct the model to (i) generate a response most likely to be upvoted, and (ii) generate a response most likely to engage the author.

\subsubsection{Experiment 2}
\textbf{Experiment 2} is designed to analyze the differences in judgments for the r/aita dataset across two different sources of responses: i. human-authored, and ii. LLM-authored responses for the two psychosocial paradigms - Rogerian PCT and Goffman's ToF.

% Additionally, the accuracy, f1... for multi class classification is reported for 2 paradigms - Rogerian PCT and Goffman's ToF.

\smallskip \textbf{Statistical Methods}

Judgments are extracted from the annotated dataset under three class labels: (i) NTA, (ii) YTA, and (iii) No (explicit) judgment. These labels are used to construct a 3×3 confusion matrix, with the human-authored judgment as the ground truth and the LLM-authored judgment as the prediction. Per-Class performance metrics and pairwise error rates, including the False Negative Rate (FNR) and False Positive Rate (FPR), are reported for each class label in Section~\ref{sec:Results}.

The measurements thus obtained are used to inform the analysis on how 'judgments' differ between human- and LLM-authored responses.

\subsubsection{Generations}
%TODO review this section for clarity and grammar. Clarify to the reader that the evaluation pipeling is first run on the basic output without any control mechanisms and then run again on the controlled output to compare and evaluate the results.
Experiments 1 and 2 are rerun, this time to investigate the efficacy of control mechanisms to align the reasoning in language model outputs more closely with that of human experts. The output response of the language model can then be run through the evaluation pipeline again to compare the distributions of values with the human-sourced responses.

The implementation of generations with the chain-of-thought reasoning is detailed under the \textbf{Generation Pipeline} subsystem described in Section~\ref{sec:Pipeline}.

\subsection{Construct Validity and Evaluation Metrics}
To assess construct validity, one human annotator labeled 100 randomly sampled post–response pairs across all four paradigms for each response type. The PCT paradigm encompasses 15 values, Goffman’s ToF 5, RVS 36, and AVT 18.

Inter-rater reliability reached Cohen’s $\kappa$ above xx for all metrics, with an overall classification accuracy of yy. For the AITAH dataset, verdicts and accompanying statements in responses were used as proxies for Empathy and Sycophancy, each mapped onto five behaviors as defined by their respective theoretical traditions\footnote{This strategy is conceptually aligned with prior work on social sycophancy \cite{cheng-etal-sycophancy}}.

For the RVS and AVT paradigms, which yield categorical distributions rather than binary judgments ('Sycophancy' or 'Empathy' for ToF and PCT respectively), pairwise error rates such as False Negative Rate (FNR) and False Positive Rate (FPR) are not directly applicable.
To identify significant associations between features annotated under more than one distinct paradigm we construct frequency tables and use chi-square analysis with the Benferroni correction with further details provided in section~\ref{sec:Analysis}. 
% TODO: Add some math here for the chi-square analysis and p-value calculations or put it in the appendix and reference here.